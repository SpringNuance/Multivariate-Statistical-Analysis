---
title: "Assignment 2"
author: "Nguyen Xuan Binh"
header-includes:
   - \usepackage{amssymb}
output:
  pdf_document:
    toc: no
    toc_depth: 1
  word_document:
    toc: no
    toc_depth: '1'
urlcolor: blue
---

# Homework Problem 2: PCA for Simulated Data

**Simulate 100 observations from bivariate normal distribution with parameters:**\

**$\quad\quad\quad\quad\quad\quad\quad\quad\mu=\begin{pmatrix} 4\\7\end{pmatrix}, \quad \Sigma=\begin{pmatrix}10 & 6\\6 & 8\end{pmatrix}.$**

```{r}
setwd(getwd())
```

---

**a) Plot the data. Label the data points with the corresponding observation number.**

```{r}
library("ggplot2")
library("mvtnorm")
```

```{r}
# Set parameters for the bivariate normal distribution
mu <- c(4, 7)
sigma <- matrix(c(10, 6, 6, 8), nrow = 2, ncol = 2)

# Simulate 100 observations
set.seed(123)
multinormData <- data.frame(rmvnorm(n = 100, mean = mu, sigma = sigma))

# Plot data and label points with observation number
library(ggplot2)
ggplot(multinormData, aes(x = X1, y = X2)) + 
  geom_point() + geom_text(hjust= 0.5, vjust=-0.5, label = 1:100, size=3) +
  ggtitle("Simulation of 100 datapoints from the bivariate normal distribution") + 
  xlab("X1") + ylab("X2") 
```


---

**b) Perform the covariance based PCA transformation to the data set.**
```{r}

multinormData.PCA <- princomp(multinormData,cor=FALSE)

# names(multinormData.PCA)

cat("sdev\n")
multinormData.PCA$sdev

cat("\nThe G matrix (columns are eigenvectors)")
multinormData.PCA$loadings

cat("\nThe Y matrix\n")
multinormData.PCA$scores

cat("\nThe sample mean\n")
multinormData.PCA$center

cat("\nRelevant when cor=TRUE\n")
multinormData.PCA$scale

cat("\nnumber of observations\n")
multinormData.PCA$n.obs

cat("\nfunction input\n")
multinormData.PCA$calls
```
---

**c) Plot the score matrix. Use the same scale as in a) and label the data points with the corresponding observation number. Choose your scale (limits for the x- and y-axis) in a way that all the observations are visible in the figure.**\
```{r}
library(ggplot2)
ggplot(data.frame(multinormData.PCA$scores), aes(x = Comp.1, y = Comp.2)) + 
  geom_point() + geom_text(hjust= 0.5, vjust=-0.5, label = 1:100, size=3) +
  ggtitle("Principle components graph") + 
  xlab("Component 1") + ylab("Component 2") 
```


---

**d) Compare the plots of a) and c) and describe the differences.**\

```{r}
# Plot data and label points with observation number
ggplot(multinormData, aes(x = X1, y = X2)) + 
  geom_point() + geom_text(hjust= 0.5, vjust=-0.5, label = 1:100, size=3) +
  ggtitle("Simulation of 100 datapoints from the bivariate normal distribution") + 
  xlab("X1") + ylab("X2") 
ggplot(data.frame(multinormData.PCA$scores), aes(x = Comp.1, y = Comp.2)) + 
  geom_point() + geom_text(hjust= 0.5, vjust=-0.5, label = 1:100, size=3) +
  ggtitle("Principal components graph") + 
  xlab("Component 1") + ylab("Component 2") 
```
We can observe that the PCA transform has rotated the original data to be orthogonal to the axis. This is because the principal components are the eigenvectors of the covariance matrix of the data, which tries to maximize the difference between the datapoints. The first component has the largest eigenvalue, the second component has the second largest eigenvalue and so on. So in short, the difference between them is that the covariance between the original data is nonzero, while the covariance under the PCA transformed graph is zero or nearly zero. \

--- 

**e)  Calculate the $G$ and $Y$ matrices without using any existing PCA functions. Note that the function princomp scales the covariance matrix with $1/n$ (instead of the usual $1/(n-1)$). Attach the R code to your solution.**\

The matrix G is the eigenvector matrix of the sample covariance matrix $\Sigma$
The matrix Y is the sample PCA transformation: $Y = (X - 1_n\overline{x}^T)G$

Model reference
```{r}
cat("\nThe G matrix (columns are eigenvectors)")
multinormData.PCA$loadings

cat("\nThe Y matrix\n")
multinormData.PCA$scores
```

```{r}
calculateG <- function(multinormData){
  covarianceMatrix <- cov(multinormData)
  G <- eigen(covarianceMatrix)$vectors
  return(G)
}

calculateY <- function(multinormData, G){
  n <- nrow(multinormData)
  meanData <- colMeans(multinormData)
  centered <- sweep(multinormData, 2, meanData, "-")
  Y <- as.matrix(centered) %*% (G) 
  return(Y)
}

G <- calculateG(multinormData)
cat("The matrix G is\n")
print(G)

Y <- calculateY(multinormData, G)
cat("\nThe matrix Y is\n")
print(Y)
```
Note that the sign of the eigenvectors does not matter. Therefore, the matrix G produced here matches the referenced G matrix above by putting minus sign on the first component\

---

**f) Verify that the estimated scores and the loadings are equal (up to signs) in parts b) and e). Hint: If parts b) and e) are done correctly, the scores and loadings should be the same up to heterogeneous sign changes.**\

Yes, they are equal, please scroll up to check part (b) and (e) again. The matrix G is the loadings matrix and the matrix Y is the scores matrix. Together, they make up the PCA transformation matrix, where the G * Y = PCA.\

---

**g) Plot the directions of the first and second principal component to the original data. The function arrows might be useful.**\

```{r}
x <- multinormData.PCA$center["X1"]
y <- multinormData.PCA$center["X2"]
eigenVectorComponent1 <- loadings(multinormData.PCA)[1:2, 1]
eigenVectorComponent2 <- loadings(multinormData.PCA)[1:2, 2]

# Plot data and label points with observation number
ggplot(multinormData, aes(x = X1, y = X2)) + 
  geom_point() +
  ggtitle("Principle component vectors of the bivariate normal distribution") + 
  xlab("X1") + ylab("X2") + 
  geom_segment(aes(x = x, y = y, xend = x + 2 * eigenVectorComponent1[1], 
                   yend = y + 2 * eigenVectorComponent1[2]),
                arrow = arrow(length = unit(0.2, "cm"), type = "closed"),
                color = "red", linewidth = 1) +
  annotate("text", x = x + 2 * eigenVectorComponent1[1] + 1, y = y + 2 *    
             eigenVectorComponent1[2] + 0.5, label = "First component", 
           color = "red", size = 5) +   
  geom_segment(aes(x = x, y = y, xend = x + 2 * eigenVectorComponent2[1], 
                   yend = y + 2 * eigenVectorComponent2[2]),
                arrow = arrow(length = unit(0.2, "cm"), type = "closed"),
                color = "blue", lindwidth = 1) +
  annotate("text", x = x + 2 * eigenVectorComponent2[1] + 2, y = y + 2 * 
             eigenVectorComponent2[2] - 1, label = "Second component", 
           color = "blue", size = 5)
  
```

